{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNX9LHe9Vre007+Dd1ysfYH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yamini542/Applied-AI/blob/main/ai_week2_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLsgbs4umAan",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e55c3a60-86d6-4c2a-83ab-b49bea75f0a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Error loading averaged_percentage_tagger: Package\n",
            "[nltk_data]     'averaged_percentage_tagger' not found in index\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ],
      "source": [
        "#Natural Language Processing\n",
        "import nltk \n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_percentage_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "import en_core_web_sm \n",
        "nlp=en_core_web_sm.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenisation\n",
        "\n",
        "how to tokenize the text "
      ],
      "metadata": {
        "id": "ZX6b3y0FqTFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text=\"This is a sentence which contains all kinds of words, and needs to be tokenized\"\n",
        "sample_tweet1=\"This is a cool:-) :-p<3 #cool\"\n",
        "sample_tweet2=\"@remy:This is a wayy too much for you !!!!\"\n"
      ],
      "metadata": {
        "id": "ieuRGJrGoPis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text=word_tokenize(sample_text)\n",
        "print(tokenized_text)\n"
      ],
      "metadata": {
        "id": "1nv6wB9frCUM",
        "outputId": "ed1a2838-954c-497e-bc1f-01d47980306a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sentence', 'which', 'contains', 'all', 'kinds', 'of', 'words', ',', 'and', 'needs', 'to', 'be', 'tokenized']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_tweet1 = word_tokenize(sample_tweet1)\n",
        "print(f'tokenized tweet1: {tokenized_tweet1}')\n",
        "\n",
        "tokenized_tweet2 = word_tokenize"
      ],
      "metadata": {
        "id": "cnyCNcNLrP3C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}